import torch
import torch.autograd

from model import loss
from PIL import Image


def adjust_learning_rate(optimizer, epoch, base_lr):
    lr = base_lr * (0.1 ** (epoch // 10))
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr

def to_variable(x):
    return torch.autograd.Variable(x.cuda() if torch.cuda.is_available() else x)

def to_numpy(x):
    return x.cpu().data.numpy() if torch.cuda.is_available() else x.data.numpy()

class GANTrainer:
    def __init__(self, generator_ab, generator_ba, discriminator_a, discriminator_b, class_a_data_loader,
                 class_b_data_loader, config):

        self.genAB, self.genBA = generator_ab, generator_ba
        self.discA, self.discB = discriminator_a, discriminator_b
        self.dataA, self.dataB = class_a_data_loader, class_b_data_loader
        self.config = config

        self.base_lr = 0.0001
        self.gen_optimizer = torch.optim.Adam(list(self.genAB.parameters()) + list(self.genBA.parameters()), lr=0.00001,
                                              betas=[0.5, 0.99])
        self.disc_optimizer = torch.optim.Adam(list(self.discA.parameters()) + list(self.discB.parameters()), lr=0.00001,
                                               betas=[0.5, 0.99])

        if torch.cuda.is_available():
            self.genAB.cuda()
            self.genBA.cuda()
            self.discA.cuda()
            self.discB.cuda()

    def train(self):
        a_iterator, b_iterator = iter(self.dataA), iter(self.dataB)
        n_iterations = min(len(a_iterator), len(b_iterator))

        for n_epoch in range(0, 100):
            avg_real_loss, avg_fake_loss = 0, 0
            avg_a_b_loss, avg_b_a_loss = 0, 0
            for n_iter in range(0, n_iterations):
                try:
                    a_data, a_label = next(a_iterator)
                    a_data, a_label = to_variable(a_data), to_variable(a_label)

                    b_data, b_label = next(b_iterator)
                    b_data, b_label = to_variable(b_data), to_variable(b_label)

                    # Train the Discriminators with real images in their classes
                    self.reset_optimizers()
                    disc_loss = loss.discriminator_loss(self.discA(a_data), true_label=1) + \
                                loss.discriminator_loss(self.discB(b_data), true_label=1)
                    disc_loss.backward()
                    avg_real_loss += disc_loss
                    self.disc_optimizer.step()

                    # Train the Discriminators with images generated by the Generators
                    self.reset_optimizers()
                    disc_loss = loss.discriminator_loss(self.discB(self.genAB(a_data)), true_label=0) + \
                                loss.discriminator_loss(self.discA(self.genBA(b_data)), true_label=0)
                    disc_loss.backward()
                    avg_fake_loss = disc_loss
                    self.disc_optimizer.step()

                    # Train network on A to B to A cycle
                    self.reset_optimizers()
                    generated_b = self.genAB(a_data)
                    disc_b_output = self.discB(generated_b)
                    reconstructed_a = self.genBA(generated_b)
                    total_loss = loss.discriminator_loss(disc_b_output, true_label=1) + \
                                 loss.discriminator_loss(a_data - reconstructed_a, true_label=0)
                    total_loss.backward()
                    avg_a_b_loss += total_loss
                    self.gen_optimizer.step()

                    # Train network on B to A to B cycle
                    self.reset_optimizers()
                    generated_a = self.genBA(b_data)
                    disc_a_output = self.discA(generated_a)
                    reconstructed_b = self.genAB(generated_a)
                    total_loss = loss.discriminator_loss(disc_a_output, true_label=1) + \
                                 loss.discriminator_loss(b_data - reconstructed_b, true_label=0)
                    total_loss.backward()
                    avg_b_a_loss += total_loss
                    self.gen_optimizer.step()

                    if n_iter % 20 == 0:
                        print("{}.{} complete.".format(n_epoch, n_iter))
                except StopIteration:
                    break

            print('EPOCH {}'.format(n_epoch))
            print("   Average real loss: {}".format(avg_real_loss/n_iterations))
            print("   Average fake loss: {}".format(avg_fake_loss/n_iterations))
            print("   Average A->B->A loss: {}".format(avg_a_b_loss/n_iterations))
            print("   Average B->A->B loss: {}".format(avg_b_a_loss/n_iterations))

            # Redeclare iterators in order to train over the set again
            a_iterator, b_iterator = iter(self.dataA), iter(self.dataB)

            # Save the model each epoch
            torch.save(self.genAB.state_dict(), './data/models/{}_genAB.pkl'.format(n_epoch))
            torch.save(self.genBA.state_dict(), './data/models/{}_genBA.pkl'.format(n_epoch))
            torch.save(self.discA.state_dict(), './data/models/{}_discA.pkl'.format(n_epoch))
            torch.save(self.discB.state_dict(), './data/models/{}_discB.pkl'.format(n_epoch))

            self.update_optimizers(n_epoch)

    def reset_optimizers(self):
        self.gen_optimizer.zero_grad()
        self.disc_optimizer.zero_grad()

    def update_optimizers(self, epoch):
        adjust_learning_rate(self.gen_optimizer, epoch, self.base_lr)
        adjust_learning_rate(self.gen_optimizer, epoch, self.base_lr)